{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "# !pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndef fill_events(df):\\n    transitions = df[df.start_anno.isnull()==False][['start_anno','stop_anno']]\\n    transitions.sort(inplace=True)\\n\\n    # Fill in event types\\n    df['anno'] = [np.nan]*len(df)\\n    df.anno[:transitions.index[0]] = transitions.start_anno[0]\\n    df.anno[transitions.index[-1]:] = transitions.stop_anno[-1]\\n\\n    # Set the annotations\\n    for i in range(len(transitions)-1):\\n        tl,tr = transitions.iloc[i],transitions.iloc[i+1]\\n        df.anno[tl.name:tr.name] = tl.start_anno\\n    return df\\n    \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from os.path import join, isdir\n",
    "from glob import glob\n",
    "import pytz\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", \n",
    "        rc={'image.cmap':'viridis','font.size':40, 'pdf.fonttype':42, 'image.interpolation':'none'}, \n",
    "        font_scale=1.5)\n",
    "loadx = lambda f: pd.read_csv(f)\n",
    "\n",
    "'''\n",
    "def fill_events(df):\n",
    "    transitions = df[df.start_anno.isnull()==False][['start_anno','stop_anno']]\n",
    "    transitions.sort(inplace=True)\n",
    "\n",
    "    # Fill in event types\n",
    "    df['anno'] = [np.nan]*len(df)\n",
    "    df.anno[:transitions.index[0]] = transitions.start_anno[0]\n",
    "    df.anno[transitions.index[-1]:] = transitions.stop_anno[-1]\n",
    "\n",
    "    # Set the annotations\n",
    "    for i in range(len(transitions)-1):\n",
    "        tl,tr = transitions.iloc[i],transitions.iloc[i+1]\n",
    "        df.anno[tl.name:tr.name] = tl.start_anno\n",
    "    return df\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = join(\"C:/\", \"Users\", \"Owner\", \"Documents\", \"Baker Lab\", \"Embrace\", \"Data\", \"Data\", \"half1\", \"F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patient_dirs = [d for d in glob(join(base_dir,'*')) if isdir(d)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send help: What is patient_dirs supposed to be exactly? A folder? All folders in a larger folder? Where was that larger folder indicated? Is it base_dir?  What is base_dir?\n",
    "\n",
    "Also, what's the best way to convert it such that it looks at *my* directories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transall is the file name? make sure that there's only one.\n",
    "# All the actigraphy files are split into multiple sessions: that's what the joining does: puts them together into one file\n",
    "# Not putting them together, but keeping them together in the same structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the annotation & actigraphy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for pdir in patient_dirs:\n",
    "    patient = pdir.split(\"/\")[-1]\n",
    "    event_file = glob(join(pdir,'redcap/processed/*trans_all*')) or [\"\"]\n",
    "    assert len(event_file) == 1, \"Should only be one summary file\"\n",
    "    event_file = event_file[0]\n",
    "    if not event_file:\n",
    "        continue\n",
    "    acc_files = glob(join(pdir,'actigraphy/raw/*acc*'))\n",
    "    eda_files = glob(join(pdir,'actigraphy/raw/*eda*'))\n",
    "    temp_files = glob(join(pdir,'actigraphy/raw/*temp*'))\n",
    "    report_files = glob(join(pdir,'actigraphy/raw/*report*'))\n",
    "    \n",
    "    if not (len(acc_files) and len(eda_files) and len(temp_files) and len(report_files)):\n",
    "        continue\n",
    "\n",
    "    metadata.append(\n",
    "        dict(patient=patient,\n",
    "            event_file=event_file,\n",
    "            patient_dir=pdir,\n",
    "            acc_files=acc_files,\n",
    "            eda_files=eda_files,\n",
    "            temp_files=temp_files,\n",
    "            report_files=report_files)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas is like excel but better -- convert to pandas asap\n",
    "# load is just from the beginning: solves typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the annotation & actigraphy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:03<00:00,  4.00s/it]\n"
     ]
    }
   ],
   "source": [
    "for m in tqdm(metadata):\n",
    "    \n",
    "    # Load the actigraphy\n",
    "    # Load everything, concatenate files. Adds keys into each dictionary.\n",
    "    m['accdf'] = pd.concat([loadx(acc_file) for acc_file in m['acc_files']],axis=0)\n",
    "    m['edadf'] = pd.concat([loadx(eda_file) for eda_file in m['eda_files']],axis=0)\n",
    "    m['tempdf'] = pd.concat([loadx(temp_file) for temp_file in m['temp_files']],axis=0)\n",
    "    m['actdf'] = pd.concat([loadx(temp_file) for temp_file in m['report_files']],axis=0)\n",
    "\n",
    "    # Load the events\n",
    "    # 'event_file' is a csv style, read it as a string, parse it, care only about certain keys\n",
    "    # Delete new lines, split by spaces, extract fields, extracting annotations\n",
    "    events = []\n",
    "    with open(m['event_file'],'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines[1:]:\n",
    "            entry = dict()\n",
    "            l = l.replace('\\n','')\n",
    "            fields = l.split(' ')\n",
    "            entry['patient'],entry['YMD'],entry['HMS'] = fields[:3]\n",
    "            anno = ' '.join(fields[3:])\n",
    "            start,stop = anno.replace('{','').replace('}','').split(':',maxsplit=1)\n",
    "            start = start.split(',')\n",
    "            stop = stop.split(',')\n",
    "            entry['start_anno'],entry['start_rest'] = start[0],start[1:]\n",
    "            entry['stop_anno'],entry['stop_rest'] = stop[0],stop[1:]\n",
    "            events.append(entry)\n",
    "\n",
    "    m['eventdf'] = pd.DataFrame(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dates right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:44<00:00, 45.00s/it]\n"
     ]
    }
   ],
   "source": [
    "tz = pytz.timezone('America/New_York')\n",
    "for m in tqdm(metadata):\n",
    "\n",
    "    # Get the dates right on the events\n",
    "    hour,minute,second = zip(*[map(int,hms.split(':')) for hms in m['eventdf']['HMS']])\n",
    "    year,month,day = zip(*[map(int,ymd.split('-')) for ymd in m['eventdf']['YMD']])\n",
    "    dates = [\n",
    "        pytz.datetime.datetime(year=year_,\n",
    "                               month=month_,\n",
    "                               day=day_,\n",
    "                               hour=hour_,\n",
    "                               minute=minute_,\n",
    "                               second=second_,\n",
    "                               tzinfo=tz)\n",
    "        for year_,month_,day_,hour_,minute_,second_\n",
    "        in zip(year,month,day,hour,minute,second)]\n",
    "    m['eventdf'].insert(0,'ts',dates)\n",
    "    del m['eventdf']['HMS']\n",
    "    del m['eventdf']['YMD']\n",
    "\n",
    "    # Get the dates right on the actigraphy report\n",
    "    def convert_actigraphy_report_ts(str_t):\n",
    "        d = pytz.datetime.datetime.strptime(str_t,'%Y-%m-%d %H:%M:%S')\n",
    "        return d.replace(tzinfo=tz)\n",
    "    m['actdf'].insert(0,'ts',\n",
    "                      [convert_actigraphy_report_ts(t)\n",
    "                       for t in m['actdf']['Timestamp (UTC)']])\n",
    "    del m['actdf']['Timestamp (UTC)']\n",
    "    del m['actdf']['Timezone offset']\n",
    "\n",
    "    # Get the dates right on the actigraphy raw data\n",
    "    m['edadf'].insert(0,'ts',\n",
    "                      [pytz.datetime.datetime.utcfromtimestamp(t/1e3).replace(tzinfo=tz)\n",
    "                     for t in m['edadf']['timestamp_milliseconds']])\n",
    "    del m['edadf']['timestamp_milliseconds']\n",
    "\n",
    "    m['accdf'].insert(0,'ts',\n",
    "                      [pytz.datetime.datetime.utcfromtimestamp(t/1e3).replace(tzinfo=tz)\n",
    "                     for t in m['accdf']['timestamp_milliseconds']])\n",
    "    del m['accdf']['timestamp_milliseconds']\n",
    "\n",
    "    m['tempdf'].insert(0,'ts',\n",
    "                      [pytz.datetime.datetime.utcfromtimestamp(t/1e3).replace(tzinfo=tz)\n",
    "                     for t in m['tempdf']['timestamp_milliseconds']])\n",
    "    del m['tempdf']['timestamp_milliseconds']\n",
    "    \n",
    "    # Make sure to verify this cell. It's incredibly important: any error leads to more errors in the future\n",
    "    # Eventually: all things recorded from devices get an index (starting time) from the same timezone\n",
    "\n",
    "    m['eventdf'].set_index('ts',inplace=True)\n",
    "    m['actdf'].set_index('ts',inplace=True)\n",
    "    m['edadf'].set_index('ts',inplace=True)\n",
    "    m['accdf'].set_index('ts',inplace=True)\n",
    "    m['tempdf'].set_index('ts',inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in metadata:\n",
    "    m['actdf'] = m['actdf'].rename(\n",
    "    columns={\n",
    "        'Acceleration magnitude [normalised by g. The interval is between 0 and 28 g]':'mean_acc_magnitude',\n",
    "        'Skin temperature':'skin_temperature',\n",
    "        'EDA':'eda',\n",
    "        'MET':'met'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fix the stuff\n",
    "# Add in acceleration magnitude\n",
    "# adding things can go here\n",
    "for m in tqdm(metadata):\n",
    "    m['accdf']['acc_magnitude'] = np.sqrt(m['accdf'].X**2.0\n",
    "                                          + m['accdf'].Y**2.0\n",
    "                                          + m['accdf'].Z**2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  5.42it/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:01<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Add annotations to all sensors: events have same timestamp as the other three\n",
    "# apparently can get rid of the predefined function at the beginning\n",
    "mdf = {}\n",
    "modalities = ['edadf','tempdf','accdf']\n",
    "for modality in modalities: # for each type of analysis\n",
    "    mdf[modality] = pd.DataFrame()\n",
    "\n",
    "    for m in tqdm(metadata): # for each patient\n",
    "        # Join the current modality and events\n",
    "        left = m[modality] # accelerometer data\n",
    "        right = m['eventdf'][['patient','start_anno']] # annotations\n",
    "        df_ = left.join(right,how='outer',sort=True) # rows joined on time: chance that\n",
    "        # empatica was turned on when the person started watching is pretty much 0\n",
    "        # Therefore: df_ = df_.fillna means fill in numbers?\n",
    "\n",
    "        # Get left-most overlapping time index\n",
    "        # time when annotations start: First timestamp when someone was looking at the person\n",
    "        start = right.index[0]\n",
    "\n",
    "        # Get right-most overlapping time index\n",
    "        # Earlier time when device or annotations end\n",
    "        stop = min(left.index[-1],right.index[-1])\n",
    "\n",
    "        # Crop the joined df_\n",
    "        # basically trim everything.\n",
    "        df_ = df_[start:stop]\n",
    "\n",
    "        # Fill in the annotations so they match the \n",
    "        # time index of the data modality\n",
    "        df_['anno'] = df_['start_anno'] # What the person was doing until they were labeled to be doing something different\n",
    "        del df_['start_anno']\n",
    "        df_ = df_.fillna(method='ffill')\n",
    "        \n",
    "        mdf[modality] = pd.concat((mdf[modality],df_),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# High level goal: can we predict someone going into a vigorous state before they've done so\n",
    "# But that might not be the problem to solve: doesn't necessarily mean you need to be restrained\n",
    "# But we're trying to solve when people need to be restrained\n",
    "# And note that the annotation accuracies are kind of... eh. We need to be beating \"bored people\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# '/Users/Joanna/figure out your own way to find the path...\n",
    "# Figure out a goal that you want done at a certain date?\n",
    "# Ask Alex as a resource to check progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've done all the preprocessing we're going to do in this notebook.\n",
    "# Save it out, and we'll continue the journey elsewhere.\n",
    "from joblib import dump\n",
    "with open(r\"C:\\Users\\Owner\\Documents\\Baker Lab\\Embrace\\F1.bin\", \"wb\") as f:\n",
    "    data_file = f\n",
    "    dump(mdf,data_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
